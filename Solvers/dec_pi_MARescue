using Pkg
using Statistics
using POMDPTools
Pkg.activate("dec_pomdp_env")  # Create a new environment
# include("dec_tiger.jl")  # Your Dec-Tiger implementation file
# include("multi_agent_rescue.jl")
include("../Problems/satellite_dec_pomdp.jl")

# Define the FSC node structure
struct FSCNode
    action::Int  # Index of the action to take
    transitions::Dict{String, Int}  # Observation -> next node mapping
end

struct AgentController
    nodes::Vector{FSCNode}
end

struct JointController
    controllers::Vector{AgentController}
end


function create_heuristic_controller(prob::POMDP)
    agent_controllers = Vector{AgentController}()
    
    for i in 1:2
        # Node 1: Listen
        node1 = FSCNode(
            1,  # Listen action
            Dict("hear-left" => 2, "hear-right" => 3)  # Transition based on observation
        )
        
        # Node 2: Listen again to confirm (heard tiger on left)
        node2 = FSCNode(
            1,  # Listen action
            Dict("hear-left" => 4, "hear-right" => 1)  # Confirm or go back to listening
        )
        
        # Node 3: Listen again to confirm (heard tiger on right)
        node3 = FSCNode(
            1,  # Listen action
            Dict("hear-left" => 1, "hear-right" => 5)  # Confirm or go back to listening
        )
        
        # Node 4: Open right door (confident tiger is on left)
        node4 = FSCNode(
            3,  # Open right
            Dict("hear-left" => 1, "hear-right" => 1)  # Go back to listening
        )
        
        # Node 5: Open left door (confident tiger is on right)
        node5 = FSCNode(
            2,  # Open left
            Dict("hear-left" => 1, "hear-right" => 1)  # Go back to listening
        )
        
        ctrl = AgentController([node1, node2, node3, node4, node5])
        push!(agent_controllers, ctrl)
    end
    
    return JointController(agent_controllers)
end

function compute_observation_probability(state, joint_action, joint_obs, prob::POMDP)
    # Get all possible joint observations
    all_observations = observations(prob)
    
    # Create a temporary joint action tuple of the correct form
    joint_action_strings = Tuple(agent_actions(prob)[a] for a in joint_action)
    
    # Calculate observation probability using the model's function
    # Note: This uses the next state 'sp' which should be passed in but we're using 'state' as a simplification
    obs_distribution = POMDPs.observation(prob, joint_action_strings, state)
    
    # Find our specific joint observation in the distribution
    joint_obs_tuple = Tuple(joint_obs)
    
    # Find the probability of this specific observation
    for (idx, obs_tuple) in enumerate(all_observations)
        if obs_tuple == joint_obs_tuple
            return obs_distribution.probs[idx]
        end
    end
    
    return 0.0  # Observation not found
end

function compute_single_observation_probability(state, action, obs)
    if action == "listen"
        # Correct observation with 85% probability
        if (state == "tiger-left" && obs == "hear-left") || 
           (state == "tiger-right" && obs == "hear-right")
            return 0.85
        else
            return 0.15
        end
    else
        # After opening a door, observation probabilities are uniform
        return 0.5
    end
end

function evaluate_controller(joint_controller::JointController, problem::POMDP)
    # Get all possible states
    states = POMDPs.states(problem)
    
    # Get actions and observations for individual agents
    actions = agent_actions(problem)
    observations = agent_observations(problem)
    
    # Map states to indices for easier array access
    state_map = Dict(s => i for (i, s) in enumerate(states))
    
    # Get the number of states and number of nodes for each agent
    num_states = length(states)
    num_agents = length(joint_controller.controllers)
    nodes_per_agent = [length(c.nodes) for c in joint_controller.controllers]
    
    # Create value function matrix
    dims = [nodes_per_agent..., num_states]
    V = zeros(dims...)
    
    # Discount factor
    gamma = problem.discount_factor
    
    # Value iteration to compute the value function
    max_iterations = 1000
    epsilon = 0.001
    
    for iter in 1:max_iterations
        # Make a copy of V for updating
        V_new = copy(V)
        
        # For each possible joint node configuration and state
        for node_indices in Iterators.product([1:n for n in nodes_per_agent]...)
            for s in 1:num_states
                # Get the current state name
                state = states[s]
                
                # Joint action from node policies
                joint_action = [joint_controller.controllers[i].nodes[node_indices[i]].action 
                               for i in 1:num_agents]
                
                # Convert joint action indices to strings for the reward function
                joint_action_strings = Tuple(actions[a] for a in joint_action)
                
                # Calculate immediate reward
                immediate_reward = POMDPs.reward(problem, state, joint_action_strings)
                
                # Expected future reward
                future_reward = 0.0
                
                # For each possible next state
                for next_s in 1:num_states
                    next_state = states[next_s]
                    
                    # Get transition distribution
                    action_tuple = Tuple(actions[a] for a in joint_action)
                    trans_distribution = POMDPs.transition(problem, state, action_tuple)
                    
                    # Calculate transition probability based on distribution type
                    trans_prob = 0.0
                    
                    # Handle different distribution types
                    if typeof(trans_distribution) <: POMDPTools.Deterministic
                        # Deterministic transition
                        trans_prob = (trans_distribution.val == next_state) ? 1.0 : 0.0
                    elseif typeof(trans_distribution) <: POMDPTools.Uniform
                        # For Uniform distribution, we need to calculate manually
                        # Check if next_state is in the distribution
                        possible_states = POMDPs.states(problem)
                        if next_state in possible_states
                            trans_prob = 1.0 / length(possible_states)
                        else
                            trans_prob = 0.0
                        end
                    elseif typeof(trans_distribution) <: POMDPTools.SparseCat
                        # SparseCat distribution
                        for (idx, s_val) in enumerate(trans_distribution.vals)
                            if s_val == next_state
                                trans_prob = trans_distribution.probs[idx]
                                break
                            end
                        end
                    else
                        # Try to use pdf method
                        try
                            trans_prob = pdf(trans_distribution, next_state)
                        catch
                            @warn "Unsupported distribution type: $(typeof(trans_distribution))"
                            trans_prob = 0.0
                        end
                    end
                    
                    # Skip if transition probability is 0
                    if trans_prob ≈ 0.0
                        continue
                    end
                    
                    # Calculate future value for all possible observations
                    all_joint_obs = collect(Iterators.product([observations for _ in 1:num_agents]...))
                    
                    for joint_obs_tuple in all_joint_obs
                        # Get observation distribution
                        obs_distribution = POMDPs.observation(problem, action_tuple, next_state)
                        
                        # Calculate observation probability
                        obs_prob = 0.0
                        
                        if typeof(obs_distribution) <: POMDPTools.Deterministic
                            obs_prob = (obs_distribution.val == joint_obs_tuple) ? 1.0 : 0.0
                        elseif typeof(obs_distribution) <: POMDPTools.Uniform
                            # For Uniform, calculate manually
                            possible_obs = collect(Iterators.product([observations for _ in 1:num_agents]...))
                            if joint_obs_tuple in possible_obs
                                obs_prob = 1.0 / length(possible_obs)
                            else
                                obs_prob = 0.0
                            end
                        elseif typeof(obs_distribution) <: POMDPTools.SparseCat
                            for (idx, o_val) in enumerate(obs_distribution.vals)
                                if o_val == joint_obs_tuple
                                    obs_prob = obs_distribution.probs[idx]
                                    break
                                end
                            end
                        else
                            try
                                obs_prob = pdf(obs_distribution, joint_obs_tuple)
                            catch
                                obs_prob = 0.0
                            end
                        end
                        
                        # Skip if observation probability is 0
                        if obs_prob ≈ 0.0
                            continue
                        end
                        
                        # Determine next nodes for each agent
                        next_nodes = []
                        for i in 1:num_agents
                            current_node = joint_controller.controllers[i].nodes[node_indices[i]]
                            next_node = current_node.transitions[joint_obs_tuple[i]]
                            push!(next_nodes, next_node)
                        end
                        
                        # Future value
                        future_value = V[next_nodes..., next_s]
                        
                        # Update expected future reward
                        future_reward += trans_prob * obs_prob * future_value
                    end
                end
                
                # Total value for this configuration
                V_new[node_indices..., s] = immediate_reward + gamma * future_reward
            end
        end
        
        # Check convergence
        if maximum(abs.(V_new - V)) < epsilon
            println("Value iteration converged after $(iter) iterations")
            break
        end
        
        # Update value function for next iteration
        V = copy(V_new)
        
        if iter == max_iterations
            println("Warning: Value iteration did not converge after $(max_iterations) iterations")
        end
    end
    
    # Compute the value for the initial belief state and initial controller nodes
    initial_belief = ones(num_states) ./ num_states  # Uniform belief
    initial_nodes = [1 for _ in 1:num_agents]  # Start at first node for each agent
    
    value = sum(initial_belief[s] * V[initial_nodes..., s] for s in 1:num_states)
    
    return value, V
end

function dec_pomdp_pi(controller::JointController, prob)
    # Initialize
    it = 0
    epsilon = 0.01  # Desired precision
    R_max = find_maximum_absolute_reward(prob)
    gamma = prob.discount_factor
    
    ctrlr_t = deepcopy(controller)
    n = length(ctrlr_t.controllers)
    
    # Initial evaluation
    V_prev, _ = evaluate_controller(ctrlr_t, prob)
    println("Initial controller value: $(V_prev)")
    
    # Set initial values
    V_curr = V_prev
    improvement = Inf  # Start with infinite improvement to ensure first iteration
    
    # Main policy iteration loop with proper stopping condition
    while it < 30 && improvement > epsilon
        # [Backup and evaluate]
        for i in 1:n
            println("Backing up agent $i...")
            new_controller = improved_exhaustive_backup(
                ctrlr_t.controllers[i],
                ctrlr_t,
                i,
                prob
            )
            
            ctrlr_t.controllers[i] = new_controller
        end
        
        # Evaluate the joint controller
        V_curr, _ = evaluate_controller(ctrlr_t, prob)
        println("After backup, controller value: $(V_curr)")
        
        # Calculate improvement (absolute difference)
        improvement = abs(V_curr - V_prev)
        println("Improvement: $(improvement)")
        
        # Update previous value for next iteration
        V_prev = V_curr
        
        it += 1
        println("Completed iteration $(it)")
        
        # Additional stopping condition based on convergence formula
        if (gamma^it * R_max) < epsilon
            println("Theoretical bound reached, algorithm converged.")
            break
        end
    end
    
    # Report reason for stopping
    if it >= 30
        println("Stopped due to maximum iterations reached.")
    elseif improvement <= epsilon
        println("Stopped due to convergence (improvement below threshold).")
    end
    
    return it, ctrlr_t
end

function find_maximum_absolute_reward(prob::POMDP)
    max_abs_reward = 0.0
    
    # Iterate through all states
    for s in states(prob)
        # Iterate through all joint actions
        for a in actions(prob)
            # Calculate reward for this state-action pair
            r = reward(prob, s, a)
            
            # Update maximum if this absolute reward is higher
            if abs(r) > max_abs_reward
                max_abs_reward = abs(r)
            end
        end
    end
    
    return max_abs_reward
end

# Perform exhaustive backup on a controller
function improved_exhaustive_backup(controller::AgentController, joint_controller::JointController, agent_idx::Int, prob::POMDP, num_agents::Int=2)
    original_controller = deepcopy(controller)
    
    # Extract individual agent actions from the joint actions
    joint_actions = POMDPs.actions(prob)
    
    # Get unique actions for this specific agent
    # For a tuple with num_agents elements, extract the agent_idx element
    agent_actions = unique([a[agent_idx] for a in joint_actions])
    
    # Get individual agent observations
    joint_observations = POMDPs.observations(prob)
    agent_observations = unique([o[agent_idx] for o in joint_observations])
    
    current_nodes = length(controller.nodes)
    candidate_nodes = Vector{FSCNode}()
    
    # Generate all possible new nodes
    for action_idx in 1:length(agent_actions)
        num_observations = length(agent_observations)
        num_combinations = current_nodes^num_observations
        
        for combo_idx in 0:(num_combinations - 1)
            transitions = Dict{String, Int}()
            
            for (obs_idx, obs) in enumerate(agent_observations)
                divisor = current_nodes^(num_observations - obs_idx)
                node_idx = (combo_idx ÷ divisor) % current_nodes + 1
                transitions[obs] = node_idx
            end
            
            # Add this candidate node to our list
            push!(candidate_nodes, FSCNode(action_idx, transitions))
        end
    end
    
    # Evaluate the current controller value
    current_value, _ = evaluate_controller(joint_controller, prob)
    println("Current value before backup: $current_value")
    
    # Try each candidate node as a replacement for each existing node
    best_controller = deepcopy(controller)
    best_value = current_value
    improved = false
    
    # For each existing node in the controller
    for node_idx in 1:length(controller.nodes)
        # Try replacing with each candidate node
        for candidate in candidate_nodes
            # Create a temporary controller with this node replaced
            temp_controller = deepcopy(controller)
            
            # Replace the node
            new_nodes = Vector{FSCNode}()
            for i in 1:length(temp_controller.nodes)
                if i == node_idx
                    push!(new_nodes, candidate)
                else
                    push!(new_nodes, temp_controller.nodes[i])
                end
            end
            temp_controller = AgentController(new_nodes)
            
            # Create a temporary joint controller for evaluation
            temp_joint_controller = deepcopy(joint_controller)
            temp_joint_controller.controllers[agent_idx] = temp_controller
            
            # Evaluate this controller
            temp_value, _ = evaluate_controller(temp_joint_controller, prob)
            
            # If it's better, keep it
            if temp_value > best_value
                best_value = temp_value
                best_controller = deepcopy(temp_controller)
                improved = true
                println("Found improvement by replacing node $node_idx, new value: $temp_value")
            end
        end
    end
    
    # Try adding a new node (only if it helps and controller isn't too large)
    if !improved && length(controller.nodes) < 10 # Limit size to prevent explosion
        for candidate in candidate_nodes
            # Create a temporary controller with this node added
            temp_controller = deepcopy(controller)
            new_nodes = copy(temp_controller.nodes)
            push!(new_nodes, candidate)
            temp_controller = AgentController(new_nodes)
            
            # Create a temporary joint controller
            temp_joint_controller = deepcopy(joint_controller)
            temp_joint_controller.controllers[agent_idx] = temp_controller
            
            # Evaluate this controller
            temp_value, _ = evaluate_controller(temp_joint_controller, prob)
            
            # If it's better, keep it
            if temp_value > best_value
                best_value = temp_value
                best_controller = deepcopy(temp_controller)
                improved = true
                println("Found improvement by adding a new node, new value: $temp_value")
                # We found an improvement, so we can break early
                break
            end
        end
    end
    
    # Return the best controller found
    if improved
        println("Controller improved from $current_value to $best_value")
        return best_controller
    else
        println("No improvement found, returning original controller")
        return original_controller
    end
end


println("running")

# rescue_prob = MultiAgentRescuePOMDP(num_agents=4, num_locations=1)

# function create_rescue_heuristic_controller(prob::MultiAgentRescuePOMDP)
#     agent_controllers = Vector{AgentController}()
    
#     # Get actions and observations
#     actions_list = agent_actions(prob)
#     obs_list = agent_observations(prob)
    
#     for i in 1:prob.num_agents
#         # Simple controller: one node for searching, one for each extraction location
#         nodes = FSCNode[]
        
#         # Search node
#         search_transitions = Dict{String, Int}()
#         for obs in obs_list
#             if obs == "no-signal"
#                 search_transitions[obs] = 1  # Stay searching
#             else
#                 # Extract location from observation
#                 loc = parse(Int, last(split(obs, "-")))
#                 search_transitions[obs] = loc + 1  # Go to extraction node
#             end
#         end
        
#         push!(nodes, FSCNode(1, search_transitions))  # Search action is index 1
        
#         # Extraction nodes for each location
#         for loc in 1:prob.num_locations
#             extract_transitions = Dict{String, Int}()
#             for obs in obs_list
#                 extract_transitions[obs] = 1  # Return to search after extraction
#             end
            
#             push!(nodes, FSCNode(loc + 1, extract_transitions))  # Extract action is index loc+1
#         end
        
#         push!(agent_controllers, AgentController(nodes))
#     end
    
#     return JointController(agent_controllers)
# end

# rescue_ctrl = create_rescue_heuristic_controller(rescue_prob)

# _, rescue_controller = dec_pomdp_pi(rescue_ctrl, rescue_prob)

# println("Policy iteration completed. Analyzing final controller...")

# for (i, ctrl) in enumerate(rescue_controller.controllers)
#     println("Agent $i controller has $(length(ctrl.nodes)) nodes:")
#     for (j, node) in enumerate(ctrl.nodes)
#         action_name = agent_actions(rescue_prob)[node.action]
#         println("  Node $j: Action = $action_name")
#         println("    Transitions: $(node.transitions)")
#     end
# end

function verify_rescue_controller(joint_controller::JointController, prob::POMDP, num_episodes=10000, max_steps=20)
    total_reward = 0.0
    rewards_per_episode = []
    
    # Track statistics
    successful_extractions = 0
    failed_extractions = 0
    episodes_with_pos_reward = 0
    
    # Get actions and observations
    actions_list = agent_actions(prob)
    obs_list = agent_observations(prob)
    
    # For each episode
    for episode in 1:num_episodes
        # Initialize state randomly
        state = rand(states(prob))
        
        # Start at initial nodes (usually node 1)
        nodes = ones(Int, prob.num_agents)
        episode_reward = 0.0
        
        # Run for max_steps or until stopping condition
        for step in 1:max_steps
            # Get actions from current nodes
            action_indices = [joint_controller.controllers[i].nodes[nodes[i]].action 
                          for i in 1:prob.num_agents]
            
            # Convert action indices to strings
            joint_action_strings = Tuple(actions_list[a] for a in action_indices)
            
            # Get reward for current state and action
            step_reward = POMDPs.reward(prob, state, joint_action_strings)
            episode_reward += step_reward
            
            # Track extraction attempts
            extraction_attempts = [a != 1 for a in action_indices]  # Non-search actions
            if any(extraction_attempts)
                # Extract location number from state
                survivor_loc = parse(Int, last(split(state, "-")))
                
                # Count agents extracting at the correct location
                agents_at_correct_loc = 0
                for (i, action_idx) in enumerate(action_indices)
                    if action_idx > 1  # If extracting
                        # Extract location from action
                        action_loc = action_idx - 1
                        if action_loc == survivor_loc
                            agents_at_correct_loc += 1
                        end
                    end
                end
                
                # Successful extraction requires at least 2 agents at the correct location
                if agents_at_correct_loc >= 2
                    successful_extractions += 1
                    # Reset state after successful extraction
                    state = rand(states(prob))
                else
                    failed_extractions += 1
                end
            end
            
            # Generate observations
            observations = []
            for i in 1:prob.num_agents
                action = actions_list[action_indices[i]]
                
                if action == "search"
                    # When searching, agents get informative observations
                    survivor_loc = parse(Int, last(split(state, "-")))
                    
                    if rand() < prob.p_correct_obs
                        # Correct observation
                        obs = "signal-loc-$survivor_loc"
                    else
                        # Incorrect observation
                        if rand() < 0.5
                            # No signal
                            obs = "no-signal"
                        else
                            # Wrong location
                            wrong_loc = rand([l for l in 1:prob.num_locations if l != survivor_loc])
                            obs = "signal-loc-$wrong_loc"
                        end
                    end
                else
                    # When extracting, observations are uninformative (random)
                    possible_obs = deepcopy(obs_list)
                    obs = rand(possible_obs)
                end
                
                push!(observations, obs)
            end
            
            # Transition to next nodes
            for i in 1:prob.num_agents
                nodes[i] = joint_controller.controllers[i].nodes[nodes[i]].transitions[observations[i]]
            end
        end
        
        total_reward += episode_reward
        push!(rewards_per_episode, episode_reward)
        
        if episode_reward > 0
            episodes_with_pos_reward += 1
        end
    end
    
    # Calculate statistics
    avg_reward = total_reward / num_episodes
    std_dev = std(rewards_per_episode)
    success_rate = episodes_with_pos_reward / num_episodes
    
    println("=== Rescue Controller Verification Results ===")
    println("Average reward per episode: $avg_reward")
    println("Standard deviation: $std_dev")
    println("Episodes with positive reward: $(success_rate * 100)%")
    println("Successful extractions: $successful_extractions")
    println("Failed extraction attempts: $failed_extractions")
    
    if successful_extractions + failed_extractions > 0
        println("Success ratio: $(successful_extractions / (successful_extractions + failed_extractions))")
    else
        println("No extraction attempts were made.")
    end
    
    # Analyze controller structure
    println("\n=== Controller Structure Analysis ===")
    for i in 1:prob.num_agents
        println("Agent $i controller has $(length(joint_controller.controllers[i].nodes)) nodes")
        
        # Count action distribution
        action_counts = zeros(Int, length(actions_list))
        for node in joint_controller.controllers[i].nodes
            action_counts[node.action] += 1
        end
        
        for a in 1:length(actions_list)
            action_name = actions_list[a]
            percentage = action_counts[a] / length(joint_controller.controllers[i].nodes) * 100
            println("  $action_name: $(round(percentage, digits=1))%")
        end
        
        # Analyze transition patterns
        println("  Transition patterns:")
        for obs in obs_list
            obs_count = Dict{Int, Int}()
            for node in joint_controller.controllers[i].nodes
                next_node = node.transitions[obs]
                obs_count[next_node] = get(obs_count, next_node, 0) + 1
            end
            
            println("    On $obs: ", join(["$(count) to node $(node)" for (node, count) in obs_count], ", "))
        end
    end
    
    # Analyze coordination
    println("\n=== Coordination Analysis ===")
    # Count how often agents would choose to extract at the same location
    coordination_count = Dict{Tuple{Int, Int}, Int}()
    
    for node_indices in Iterators.product([1:length(c.nodes) for c in joint_controller.controllers]...)
        # Get actions from these nodes
        actions = [joint_controller.controllers[i].nodes[node_indices[i]].action 
                   for i in 1:prob.num_agents]
        
        # Check if multiple agents are extracting at the same location
        extraction_locations = [a - 1 for a in actions if a > 1]
        
        if length(extraction_locations) > 1
            # Count pairs of agents at each location
            for loc in unique(extraction_locations)
                count = sum(1 for l in extraction_locations if l == loc)
                if count >= 2
                    key = (loc, count)
                    coordination_count[key] = get(coordination_count, key, 0) + 1
                end
            end
        end
    end
    
    if !isempty(coordination_count)
        for ((loc, count), occurrences) in coordination_count
            println("  $count agents extracting at location $loc: $occurrences possible combinations")
        end
    else
        println("  No coordination patterns detected.")
    end
    
    return avg_reward, success_rate
end

# verify_rescue_controller(rescue_controller, rescue_prob)

# function verify_satellite_controller(joint_controller::JointController, prob::SatelliteNetworkPOMDP, num_episodes=1000, max_steps=50)
#     total_reward = 0.0
#     rewards_per_episode = []
#     steps_to_transmit = []
#     transmission_success_count = 0
    
#     # Track statistics for each satellite
#     satellite_transmission_attempts = zeros(Int, prob.num_satellites)
#     satellite_transmission_successes = zeros(Int, prob.num_satellites)
#     satellite_pass_attempts = zeros(Int, prob.num_satellites)
#     satellite_pass_successes = zeros(Int, prob.num_satellites)
    
#     # For each episode
#     for episode in 1:num_episodes
#         # Initialize state - data at satellite 1
#         state = "data-at-1"
        
#         # Start with first node for each satellite
#         nodes = ones(Int, prob.num_satellites)
#         episode_reward = 0.0
#         step_count = 0
        
#         # Run episode until max steps or transmission
#         while step_count < max_steps && state != "data-transmitted"
#             step_count += 1
            
#             # Get data-holding satellite
#             data_sat = parse(Int, last(split(state, "-")))
            
#             # Get actions from current nodes
#             action_indices = [joint_controller.controllers[i].nodes[nodes[i]].action 
#                              for i in 1:prob.num_satellites]
            
#             # Convert action indices to strings
#             actions = []
#             for i in 1:prob.num_satellites
#                 sat_actions = agent_actions(prob, i)
#                 push!(actions, sat_actions[action_indices[i]])
#             end
#             joint_action = Tuple(actions)
            
#             # Get reward
#             step_reward = POMDPs.reward(prob, state, joint_action)
#             episode_reward += step_reward
            
#             # Track action statistics
#             data_sat_action = actions[data_sat]
#             if data_sat_action == "transmit"
#                 satellite_transmission_attempts[data_sat] += 1
                
#                 # Check if transmission successful (based on probability)
#                 if rand() < prob.ground_tx_probs[data_sat]
#                     satellite_transmission_successes[data_sat] += 1
#                     state = "data-transmitted"
#                 end
#             elseif data_sat_action == "pass-left" && data_sat > 1
#                 satellite_pass_attempts[data_sat] += 1
                
#                 # Check if pass successful
#                 if rand() < prob.pass_success_prob
#                     satellite_pass_successes[data_sat] += 1
#                     state = "data-at-$(data_sat-1)"
#                 end
#             elseif data_sat_action == "pass-right" && data_sat < prob.num_satellites
#                 satellite_pass_attempts[data_sat] += 1
                
#                 # Check if pass successful
#                 if rand() < prob.pass_success_prob
#                     satellite_pass_successes[data_sat] += 1
#                     state = "data-at-$(data_sat+1)"
#                 end
#             end
            
#             # Generate observations for each satellite
#             observations = ["no-data" for _ in 1:prob.num_satellites]
#             if state != "data-transmitted"
#                 next_data_sat = parse(Int, last(split(state, "-")))
#                 observations[next_data_sat] = "has-data"
#             end
            
#             # Update node for each satellite
#             for i in 1:prob.num_satellites
#                 nodes[i] = joint_controller.controllers[i].nodes[nodes[i]].transitions[observations[i]]
#             end
#         end
        
#         # Record episode statistics
#         push!(rewards_per_episode, episode_reward)
#         if state == "data-transmitted"
#             transmission_success_count += 1
#             push!(steps_to_transmit, step_count)
#         end
        
#         total_reward += episode_reward
#     end
    
#     # Calculate overall statistics
#     avg_reward = total_reward / num_episodes
#     std_dev = std(rewards_per_episode)
#     success_rate = transmission_success_count / num_episodes
#     avg_steps = isempty(steps_to_transmit) ? NaN : mean(steps_to_transmit)
    
#     println("=== Satellite Network Controller Verification Results ===")
#     println("Average reward per episode: $avg_reward")
#     println("Standard deviation: $std_dev")
#     println("Transmission success rate: $(success_rate * 100)%")
#     println("Average steps to successful transmission: $avg_steps")
    
#     println("\n=== Satellite Transmission Statistics ===")
#     for i in 1:prob.num_satellites
#         tx_success_rate = satellite_transmission_attempts[i] > 0 ? 
#             satellite_transmission_successes[i] / satellite_transmission_attempts[i] : 0.0
#         pass_success_rate = satellite_pass_attempts[i] > 0 ? 
#             satellite_pass_successes[i] / satellite_pass_attempts[i] : 0.0
        
#         println("Satellite $i (Ground TX prob: $(prob.ground_tx_probs[i])):")
#         println("  Transmission attempts: $(satellite_transmission_attempts[i])")
#         println("  Transmission successes: $(satellite_transmission_successes[i]) ($(tx_success_rate*100)%)")
#         println("  Pass attempts: $(satellite_pass_attempts[i])")
#         println("  Pass successes: $(satellite_pass_successes[i]) ($(pass_success_rate*100)%)")
#     end
    
#     println("\n=== Controller Structure Analysis ===")
#     for i in 1:prob.num_satellites
#         println("Satellite $i controller has $(length(joint_controller.controllers[i].nodes)) nodes")
        
#         # Analyze node actions
#         for (j, node) in enumerate(joint_controller.controllers[i].nodes)
#             sat_actions = agent_actions(prob, i)
#             action_name = sat_actions[node.action]
#             println("  Node $j: Action = $action_name")
#             println("    Transitions: $(node.transitions)")
#         end
#     end
    
#     return avg_reward, success_rate
# end

# # Create the problem
# sat_network = SatelliteNetworkPOMDP(
#     num_satellites = 5,
#     ground_tx_probs = [0.3, 0.5, 0.6, 0.3, 0.99],
#     discount_factor = 0.9,
#     pass_success_prob = 0.95,
#     successful_tx_reward = 10.0,
#     unsuccessful_tx_penalty = -9.0,
#     pass_cost = -1.0
# )

# # Create initial controller
# initial_ctrl = create_initial_controllers(sat_network)

# # Run policy iteration
# iterations, final_controller = dec_pomdp_pi(initial_ctrl, sat_network)

# # Verify the controller
# avg_reward, success_rate = verify_satellite_controller(final_controller, sat_network)

# println("Policy iteration completed in $iterations iterations.")
# println("Final controller average reward: $avg_reward")
# println("Successful transmission rate: $(success_rate * 100)%")